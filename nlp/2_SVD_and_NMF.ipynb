{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-SVD-and-NMF.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFIhpFZPiGXX8OLa+TZpcO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leivakuro765/learm-machine-learning/blob/main/nlp/2_SVD_and_NMF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### \n",
        "\n",
        "Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF)"
      ],
      "metadata": {
        "id": "1FT1r32tR6-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RECURSO\n",
        "[ truncated SVD LSI details](https://scikit-learn.org/stable/modules/decomposition.html#lsa)\n",
        "\n",
        "[The 20 newsgroups text dataset¬∂](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\n"
      ],
      "metadata": {
        "id": "umLp5M1R__Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "only cumpute the **k** largest singular values where **K** is a user-specified parameter.\n",
        "\n",
        "ùëø ‚âà ùëø‚Çñ = U‚ÇñE‚ÇñV‚ÇñV·µÄ  \n",
        "U‚ÇñE‚Çñ\n",
        "\n",
        "the use SVD is that in the effects of synonymy and polysemy (both of wihch roughly meab there are multiple meanings per word )"
      ],
      "metadata": {
        "id": "gNS4wvTa8a8D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlrXvWteRrvH"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import linalg\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn import decomposition\n",
        "\n",
        "# config\n",
        "\n",
        "%matplotlib inline\n",
        "np.set_printoptions(suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the dataset to be used will be [fetch_20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)"
      ],
      "metadata": {
        "id": "OGUa6ter3MBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is organized into **20 different newsgroups**, each corresponding to a different topic. Some of the newsgroups are closely related to each other (eg comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are completely unrelated (eg misc .forsale/soc.religion.christian). Here's a list of the 20 newsgroups, divided (more or less) by topic:"
      ],
      "metadata": {
        "id": "Q-GeRrsp7GTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catg = ['alt.atheism', \n",
        "        'talk.religion.misc',\n",
        "        'comp.graphics',\n",
        "        'sci.space']\n",
        "train_new =fetch_20newsgroups(subset='train', categories=catg) \n",
        "test_new =fetch_20newsgroups(subset='test', categories=catg) \n",
        "\n",
        "list(train_new.target_names)"
      ],
      "metadata": {
        "id": "yX9pqn7BXF61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(test_new.target_names)\n"
      ],
      "metadata": {
        "id": "_Zl3vYPz6DO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_new.filenames.shape, train_new.target.shape,\n",
        "test_new.filenames.shape, test_new.target.shape)\n"
      ],
      "metadata": {
        "id": "uN7rt5xWBULz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n'.join(train_new.data[5:7]))"
      ],
      "metadata": {
        "id": "oxQMRxUrBvA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[_stop_words](https://stackoverflow.com/questions/68620436/cannot-import-name-stop-words-from-sklearn-feature-extraction)"
      ],
      "metadata": {
        "id": "X6elPt_GJKdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "(list(_stop_words.ENGLISH_STOP_WORDS))[:20]"
      ],
      "metadata": {
        "id": "ZeKBTmM_DbH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "For grammatical reasons of a document, different ways of representing a text will be used.\n",
        "\n",
        "the goal of lemmalization is to reduce the inflectional and sometimes derivative forms of a common base-form pala"
      ],
      "metadata": {
        "id": "q76f5aVRLAsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common algorithm for deriving English, and one that has repeatedly been shown to be very effective empirically, is [Porter's] Algorithm(https://tartarus.org/martin/PorterStemmer/). Not only do you use Porter's algorithm, there are others but they do not give a good result like porter's despite being a bit simpler in this [document](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1. html) two more are shown"
      ],
      "metadata": {
        "id": "IsSbK15HPwVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import stem\n",
        "\n",
        "def lem(word):\n",
        "  \"\"\"\n",
        "  parm:\n",
        "    word_list:: list is word list \n",
        "  return:\n",
        "    tuple the two size \n",
        "    0) lemalizado with WordNet\n",
        "    2) lemalizado with algoritmo Porter\n",
        "\n",
        "  \"\"\"\n",
        "  wnl = stem.WordNetLemmatizer()\n",
        "  porter = stem.PorterStemmer()\n",
        "  rest = (\n",
        "      [wnl.lemmatize(word) for word in word], \n",
        "      [porter.stem(word) for word in word]\n",
        "      ) \n",
        "\n",
        "  return rest\n",
        "\n",
        "foot =  ['feet','foot', 'foots','footing']\n",
        "\n",
        "print('WordNet: ',lem(foot)[0])\n",
        "print('Porter:  ',lem(foot)[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "Gh4md7SYFRGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[spacy](https://spacy.io/)"
      ],
      "metadata": {
        "id": "Y1SLjfzQ9pwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "# You will then need to download the English model:\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp('feet foot foots footing')\n",
        "\n",
        "[token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "9T0U1F_nuj4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# size stop word is 326\n",
        "(sorted(list(nlp.Defaults.stop_words))[:20],\n",
        "len(nlp.Defaults.stop_words) )\n",
        " "
      ],
      "metadata": {
        "id": "ljpl3DFi4cOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# size stop word is 179\n",
        "(sorted(nltk.corpus.stopwords.words('english'))[:20]\n",
        ",len(nltk.corpus.stopwords.words('english')))"
      ],
      "metadata": {
        "id": "H3wEx_JJ2JMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# size stop word is 318\n",
        "(sorted(list(_stop_words.ENGLISH_STOP_WORDS))[:20]\n",
        ",len(_stop_words.ENGLISH_STOP_WORDS))"
      ],
      "metadata": {
        "id": "H1zRvcE5_Iob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing\n",
        "\n",
        "After playing for a while with the modules that we will use, it is time to process the data"
      ],
      "metadata": {
        "id": "t9GiHcKUA2fL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "vectors = vectorizer.fit_transform(train_new.data).todense()\n",
        "vectors.shape"
      ],
      "metadata": {
        "id": "pB0comipAbEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_new.data), vectors.shape)\n",
        "vocab = np.array(vectorizer.get_feature_names())\n",
        "vocab.shape"
      ],
      "metadata": {
        "id": "Ht6MsnTyD1Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[200:260]"
      ],
      "metadata": {
        "id": "xlZOE8_gEOht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time U, s, Vh = linalg.svd(vectors, full_matrices=False)"
      ],
      "metadata": {
        "id": "-tWfFD1fEiPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U.shape, s.shape, Vh.shape\n",
        "plt.plot(s)"
      ],
      "metadata": {
        "id": "nuarU2EHGPQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(s[:20])"
      ],
      "metadata": {
        "id": "rRwF8ggwG0jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_words=8\n",
        "def show_topics(a):\n",
        "    global num_top_words\n",
        "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
        "    topic_words = ([top_words(t) for t in a])\n",
        "    return [' '.join(t) for t in topic_words]"
      ],
      "metadata": {
        "id": "bz6GjElcHR0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_topics(Vh[20:30])"
      ],
      "metadata": {
        "id": "sfYU3SJ0Hgm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = vectors.shape\n",
        "d = 5\n",
        "\n",
        "# model NMF\n",
        "clf = decomposition.NMF(n_components=d, random_state=1)\n",
        "W1 = clf.fit_transform(vectors)\n",
        "H1 = clf.components_\n",
        "\n",
        "show_topics(H1)"
      ],
      "metadata": {
        "id": "l1qrn7qUHjsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
        "vectors_tfidf = vectorizer_tfidf.fit_transform(train_new.data) # (documents, "
      ],
      "metadata": {
        "id": "maHQ4_TsI9Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_new.data[:1]"
      ],
      "metadata": {
        "id": "TNjHKu-wJfHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = clf.fit_transform(vectors_tfidf)\n",
        "H1 = clf.components_\n",
        "show_topics(H1)"
      ],
      "metadata": {
        "id": "3PKs3T3RJrYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(clf.components_[0])\n",
        "clf.reconstruction_err_"
      ],
      "metadata": {
        "id": "Kqb6oiinJ2Sg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}