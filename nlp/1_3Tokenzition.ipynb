{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.3Tokenzition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyObt5wqqJmLhICmyjckx8EE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahanMaigua/learm-machine-learning/blob/main/nlp/1_3Tokenzition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Normalization\n",
        "\n",
        "Before processing a text to perform a task, it is necessary to normalize the data (the text) in order to have a better result\n",
        "\n",
        "* Tokenizing (segmentation) Word\n",
        "* Normalization words Format\n",
        "* segmentating Sentences"
      ],
      "metadata": {
        "id": "5ZoaRx_zPTSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt'\n",
        "cont = requests.get(url)\n",
        "with open('shakes.txt', 'w+') as fp:\n",
        "  fp.write(cont.text)\n",
        "\n",
        "cont"
      ],
      "metadata": {
        "id": "gyjBx8ZMRbTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "dFuX4MVLSIPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZyC5v_iNVJ7"
      },
      "outputs": [],
      "source": [
        "\n",
        "!tr --help\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### unix tools for crude tokenization and normalization\n",
        "\n",
        "We can use `tr` to tokenize the word by changing every sequces of non-alphaments characters to newline to non-alphabet"
      ],
      "metadata": {
        "id": "3Ys_mSooTUTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tr -sc 'A-Za-z' '\\n' < shakes.txt "
      ],
      "metadata": {
        "id": "ChDAXEKSREeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tr -sc -c 'A-Za-z' '\\n' < shakes.txt | sort | uniq "
      ],
      "metadata": {
        "id": "cOV9Atf9Uzr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tr -sc 'A-Za-z' '\\n' < shakes.txt | tr A-Z a-z | sort | uniq -c"
      ],
      "metadata": {
        "id": "dBrc1tTVVccQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-r option means to sort in\n",
        "reverse order (highest-to-lowest):"
      ],
      "metadata": {
        "id": "kVJp_KOpfa4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tr -sc 'A-Za-z' '\\n' < shakes.txt | tr A-Z a-z | sort | uniq -c | sort -n -r"
      ],
      "metadata": {
        "id": "0hMjWJOjWcIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenizer can also be used to expand clitic contractions that are marked by\n",
        "apostrophes, for example, converting what’re to the two tokens what are, and\n",
        "we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only"
      ],
      "metadata": {
        "id": "_BA2uaegi70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "doc     = 'That U.S.A Poster-print const $12.40...'\n",
        "pattern = r\"\"\"(?x) # set flag to allow verbose \n",
        "    ([A-Z]\\.)+         # abbrevitions e.g U.S.A\n",
        "    | \\W+(-\\W+)*         # word with optional internal hyphens\n",
        "    | $?\\d+(\\.\\d+)?%?    # currency and porcentages e.g $12.40 20%\n",
        "    | \\.\\.\\.             # ellipsis\n",
        "    | [][\\.,'\"\\?\\(\\):-_] # these are separate tokens\n",
        "\"\"\".replace('\\n', '')\n",
        "\n",
        "nltk.word_tokenize(doc)"
      ],
      "metadata": {
        "id": "WV4k4Hf2folv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "puede que en algunos idomas separar las palabras no sea la mejor opcion como es el caso de los idomas asiaticos que al separase se convierte en otra palabra en eso caso lo mejor sera sementarlo"
      ],
      "metadata": {
        "id": "sC6RvavVvooY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### vocavulario\n",
        "al tokenizar las palabras creamoas un vocabulario de palabras para que ese vocabulario no tengas pabaras repetidas se puede utilizar el argorimo de `Byte pair encoding` \n",
        "\n",
        "este algorimo va comparado la veces que se repite A y B en un vocabulario \n",
        "\n"
      ],
      "metadata": {
        "id": "31ydJSxD0b3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### porter Stremmer\n",
        "The most common algorithm for deriving English, and one that has repeatedly been shown to be very effective empirically, is [Porter's](https://tartarus.org/martin/PorterStemmer/) Algorithm. Not only do you use Porter's algorithm, there are others but they do not give a good result like porter's despite being a bit simpler in this [document](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) two more are shown"
      ],
      "metadata": {
        "id": "NkfPjvsG46qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import stem\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def lem(word):\n",
        "  \"\"\"\n",
        "  parm:\n",
        "    word_list:: list is word list \n",
        "  return:\n",
        "    tuple the two size \n",
        "    0) lemalizado with WordNet\n",
        "    2) lemalizado with algoritmo Porter\n",
        "\n",
        "  \"\"\"\n",
        "  wnl = stem.WordNetLemmatizer()\n",
        "  porter = stem.PorterStemmer()\n",
        "  rest = (\n",
        "      [wnl.lemmatize(word) for word in word], \n",
        "      [porter.stem(word) for word in word]\n",
        "      ) \n",
        "\n",
        "  return rest\n",
        "\n",
        "foot =  ['feet','foot', 'foots','footing']\n",
        "\n",
        "print('WordNet: ',lem(foot)[0])\n",
        "print('Porter:  ',lem(foot)[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "fh9UwZVS77u7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}