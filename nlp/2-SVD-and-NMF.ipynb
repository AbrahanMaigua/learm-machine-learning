{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-SVD-and-NMF.ipynb","private_outputs":true,"provenance":[ ],"collapsed_sections":[],"authorship_tag":"ABX9TyPOq7OUjqyx8RVbXFyuaIko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name": "python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["\n","### ## \n","\n","Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF)"],"metadata":{"id":"1FT1r32tR6-a"}},{"cell_type ":"markdown","source":["### RESOURCE\n","[ truncated SVD LSI details](https://scikit-learn.org/stable/modules/decomposition.html#lsa)\n ","\n","[The 20 newsgroups text dataset¬∂](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\n"], "metadata":{"id":"umLp5M1R__Ac"}},{"cell_type":"markdown","source":["only cumpute the **k** largest singular values ‚Äã‚Äãwhere **K** is a user -specified parameter.\n","\n","ùëø ‚âà ùëø‚Çñ = U‚ÇñE‚ÇñV‚ÇñV·µÄ \n","U‚ÇñE‚Çñ\n","\n","the use SVD is that in the effects of synonymy and polysemy (both of wihch roughly meab there are multiple meanings per word )"],"metadata":{"id":"gNS4wvTa8a8D"}},{"cell_type":"code" ,"execution_count":null,"metadata":{"id":"xlrXvWteRrvH"},"outputs":[],"source":["import pprint\n","\n","import numpy as np \n","import matplotlib.pyplot as plt\n","from scipy import linalg\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn import decomposition\n","\n"," # config\n","\n","%matplotlib inline\n","np.set_printoptions(suppress=True)\n"]},{"cell_type":"markdown","source":["the dataset to be used will be [fetch_20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)"],"metadata":{"id":" OGUa6ter3MBu"}},{"cell_type":"markdown","source":["The data is organized into **20 different newsgroups**, each corresponding to a different topic. Some of the newsgroups are closely related to each other (eg comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are completely unrelated (eg misc .forsale/soc.religion.christian). Here is a list of the 20 newsgroups, divided (sort of) by topic:"],"metadata":{"id":"Q-GeRrsp7GTk"}},{"cell_type":"code", "source":["catg = ['alt.atheism',\n"," 'talk.religion.misc',\n"," 'comp.graphics',\n"," 'sci.space'] \n","train_new =fetch_20newsgroups(subset='train', categories=catg) \n","test_new =fetch_20newsgroups(subset='test', categories=catg) \n","\n","list( train_new.target_names)"],"metadata":{"id":"yX9pqn7BXF61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" list(test_new.target_names)\n"],"metadata":{"id":"_Zl3vYPz6DO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source ":["(train_new.filenames.shape, train_new.target.shape,\n","test_new.filenames.shape, test_new.target.shape)\n"],"metadata":{"id":"uN7rt5xWBULz "},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\\n'.join(train_new.data[5:7] ))"],"metadata":{"id":"oxQMRxUrBvA4"},"execution_count":null,"outputs":[]},{"ce ll_type":"markdown","source":["[_stop_words](https://stackoverflow.com/questions/68620436/cannot-import-name-stop-words-from-sklearn-feature-extraction)"], "metadata":{"id":"X6elPt_GJKdz"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n"," nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","from sklearn.feature_extraction import _stop_words\n","(list(_stop_words.ENGLISH_STOP_WORDS ))[:20]"],"metadata":{"id":"ZeKBTmM_DbH3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source": ["## Stemming and Lemmatization\n","\n","for grammatical reasons a document will use different ways to represent a text.\n","\n","the goal of stemming is to reduce the inflectional forms and sometimes derived from a common base form spade\n","\n"],"metadata":{"id":"q76f5aVRLAsm"}},{"cell_type":"markdown","source": ["The most common algorithm for deriving English, and one that has repeatedly been shown empirically to be very effective, is Algorithm of [ Porter](https://tartarus.org/martin/PorterStemmer/) not only do you use the Porter algorithm, there are others but they do not give a good result like Porter's despite being a little simpler in this [document] (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) two more are shown \n"],"metadata":{"id":"IsSbK15HPwVy" }},{"cell_type":"code","source":["from nltk import stem\n","\n","def lem(word):\n"," \"\"\"\ n"," parm:\n"," word_list:: list is word list \n"," return:\n"," tuple the two size \n"," 0) stemmed with WordNet\n"," 2 ) stemmed with Porter algorithm\n","\n"," \"\"\"\n"," wnl = stem.WordNetLemmatizer()\n"," porter = stem.PorterStemmer()\n"," rest = (\n"," [wnl.lemmatize(word) for word in word], \n"," [porter.stem(word) for word in word]\n"," ) \n","\ n"," return res
