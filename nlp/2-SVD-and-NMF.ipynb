{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-SVD-and-NMF.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOq7OUjqyx8RVbXFyuaIko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["\n","##### \n","\n","Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF)"],"metadata":{"id":"1FT1r32tR6-a"}},{"cell_type":"markdown","source":["### RECURSO\n","[ truncated SVD LSI details](https://scikit-learn.org/stable/modules/decomposition.html#lsa)\n","\n","[The 20 newsgroups text dataset¬∂](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\n"],"metadata":{"id":"umLp5M1R__Ac"}},{"cell_type":"markdown","source":["only cumpute the **k** largest singular values where **K** is a user-specified parameter.\n","\n","ùëø ‚âà ùëø‚Çñ = U‚ÇñE‚ÇñV‚ÇñV·µÄ  \n","U‚ÇñE‚Çñ\n","\n","the use SVD is that in the effects of synonymy and polysemy (both of wihch roughly meab there are multiple meanings per word )"],"metadata":{"id":"gNS4wvTa8a8D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlrXvWteRrvH"},"outputs":[],"source":["import pprint\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import linalg\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn import decomposition\n","\n","# config\n","\n","%matplotlib inline\n","np.set_printoptions(suppress=True)\n"]},{"cell_type":"markdown","source":["la dataset que se utilizara sera [fetch_20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)"],"metadata":{"id":"OGUa6ter3MBu"}},{"cell_type":"markdown","source":["Los datos est√°n organizados en **20 grupos de noticias diferentes**, cada uno correspondiente a un tema diferente. Algunos de los grupos de noticias est√°n estrechamente relacionados entre s√≠ (p. ej., comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), mientras que otros no tienen ninguna relaci√≥n (p. ej., misc.forsale / soc.religion.christian). Aqu√≠ hay una lista de los 20 grupos de noticias, divididos (m√°s o menos) seg√∫n el tema:"],"metadata":{"id":"Q-GeRrsp7GTk"}},{"cell_type":"code","source":["catg = ['alt.atheism', \n","        'talk.religion.misc',\n","        'comp.graphics',\n","        'sci.space']\n","train_new =fetch_20newsgroups(subset='train', categories=catg) \n","test_new =fetch_20newsgroups(subset='test', categories=catg) \n","\n","list(train_new.target_names)"],"metadata":{"id":"yX9pqn7BXF61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(test_new.target_names)\n"],"metadata":{"id":"_Zl3vYPz6DO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(train_new.filenames.shape, train_new.target.shape,\n","test_new.filenames.shape, test_new.target.shape)\n"],"metadata":{"id":"uN7rt5xWBULz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\\n'.join(train_new.data[5:7]))"],"metadata":{"id":"oxQMRxUrBvA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[_stop_words](https://stackoverflow.com/questions/68620436/cannot-import-name-stop-words-from-sklearn-feature-extraction)"],"metadata":{"id":"X6elPt_GJKdz"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","from sklearn.feature_extraction import _stop_words\n","(list(_stop_words.ENGLISH_STOP_WORDS))[:20]"],"metadata":{"id":"ZeKBTmM_DbH3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stemming and Lemmatization\n","\n","por razones gramaticales de un documento se utilizaran diversas formas de representar un texto.\n","\n","el objetivo de la lemalizacion es reducir la formas flexivas y aveces derivadas de una pala de forma base comun\n","\n"],"metadata":{"id":"q76f5aVRLAsm"}},{"cell_type":"markdown","source":["El algoritmo m√°s com√∫n para derivar el ingl√©s, y que repetidamente ha demostrado ser emp√≠ricamente muy efectivo, es Algoritmo de [Porter](https://tartarus.org/martin/PorterStemmer/) no solo se usas el algorimo de Porter exiten otros pero no llegan a dar un buen resultado como el de porter apesar de ser un poco mas simples en este [documente](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) se mustran dos mas \n"],"metadata":{"id":"IsSbK15HPwVy"}},{"cell_type":"code","source":["from nltk import stem\n","\n","def lem(word):\n","  \"\"\"\n","  parm:\n","    word_list:: list is word list \n","  return:\n","    tuple the two size \n","    0) lemalizado with WordNet\n","    2) lemalizado with algoritmo Porter\n","\n","  \"\"\"\n","  wnl = stem.WordNetLemmatizer()\n","  porter = stem.PorterStemmer()\n","  rest = (\n","      [wnl.lemmatize(word) for word in word], \n","      [porter.stem(word) for word in word]\n","      ) \n","\n","  return rest\n","\n","foot =  ['feet','foot', 'foots','footing']\n","\n","print('WordNet: ',lem(foot)[0])\n","print('Porter:  ',lem(foot)[1])\n","\n"],"metadata":{"id":"Gh4md7SYFRGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[spacy](https://spacy.io/)"],"metadata":{"id":"Y1SLjfzQ9pwC"}},{"cell_type":"code","source":["!pip install -U spacy\n","# You will then need to download the English model:\n","!python -m spacy download en_core_web_sm\n","\n","import spacy\n","# Load English tokenizer, tagger, parser and NER\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp('feet foot foots footing')\n","\n","[token.lemma_ for token in doc]"],"metadata":{"id":"9T0U1F_nuj4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# size stop word is 326\n","(sorted(list(nlp.Defaults.stop_words))[:20],\n","len(nlp.Defaults.stop_words) )\n"," "],"metadata":{"id":"ljpl3DFi4cOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# size stop word is 179\n","(sorted(nltk.corpus.stopwords.words('english'))[:20]\n",",len(nltk.corpus.stopwords.words('english')))"],"metadata":{"id":"H3wEx_JJ2JMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# size stop word is 318\n","(sorted(list(_stop_words.ENGLISH_STOP_WORDS))[:20]\n",",len(_stop_words.ENGLISH_STOP_WORDS))"],"metadata":{"id":"H1zRvcE5_Iob"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Processing\n","\n","despues de jugar un rato con los modulos que utilizaremos llego la hora de porcesar los datos "],"metadata":{"id":"t9GiHcKUA2fL"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","vectorizer = CountVectorizer(stop_words='english')\n","vectors = vectorizer.fit_transform(train_new.data).todense()\n","vectors.shape"],"metadata":{"id":"pB0comipAbEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_new.data), vectors.shape)\n","vocab = np.array(vectorizer.get_feature_names())\n","vocab.shape"],"metadata":{"id":"Ht6MsnTyD1Tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab[200:260]"],"metadata":{"id":"xlZOE8_gEOht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%time U, s, Vh = linalg.svd(vectors, full_matrices=False)"],"metadata":{"id":"-tWfFD1fEiPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["U.shape, s.shape, Vh.shape\n","plt.plot(s)"],"metadata":{"id":"nuarU2EHGPQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(s[:20])"],"metadata":{"id":"rRwF8ggwG0jO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_top_words=8\n","def show_topics(a):\n","    global num_top_words\n","    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n","    topic_words = ([top_words(t) for t in a])\n","    return [' '.join(t) for t in topic_words]"],"metadata":{"id":"bz6GjElcHR0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_topics(Vh[20:30])"],"metadata":{"id":"sfYU3SJ0Hgm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m, n = vectors.shape\n","d = 5\n","\n","# model NMF\n","clf = decomposition.NMF(n_components=d, random_state=1)\n","W1 = clf.fit_transform(vectors)\n","H1 = clf.components_\n","\n","show_topics(H1)"],"metadata":{"id":"l1qrn7qUHjsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n","vectors_tfidf = vectorizer_tfidf.fit_transform(train_new.data) # (documents, "],"metadata":{"id":"maHQ4_TsI9Y7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_new.data[:1]"],"metadata":{"id":"TNjHKu-wJfHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W1 = clf.fit_transform(vectors_tfidf)\n","H1 = clf.components_\n","show_topics(H1)"],"metadata":{"id":"3PKs3T3RJrYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(clf.components_[0])\n","clf.reconstruction_err_"],"metadata":{"id":"Kqb6oiinJ2Sg"},"execution_count":null,"outputs":[]}]}